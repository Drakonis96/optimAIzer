/**
 * Document Processing & RAG for Attached Files
 *
 * Reduces token usage by 80-90% when handling PDF, TXT, and MD attachments:
 * - Cleans extracted text (removes noise, normalizes whitespace)
 * - Splits documents into semantic chunks with overlap
 * - Indexes chunks using the existing TF-IDF VectorStore
 * - Retrieves only relevant chunks based on the user's query
 *
 * Small documents (< RAG_THRESHOLD_CHARS) are included in full.
 * Large documents are chunked and only top-K relevant chunks are sent.
 */

import { VectorStore } from './vectorStore';

// ---- Constants ----

/** Documents below this size are included in full (no chunking needed) */
export const RAG_THRESHOLD_CHARS = 3000;

/** Target size per chunk (characters) */
const CHUNK_SIZE = 1200;

/** Overlap between consecutive chunks to preserve context continuity */
const CHUNK_OVERLAP = 150;

/** Default number of top chunks to retrieve */
const DEFAULT_TOP_K = 5;

/** Maximum total characters of document context sent to the LLM */
const MAX_RAG_CONTEXT_CHARS = 8000;
const ATTACHMENT_MARKER_PATTERN = /^\s*(?:[ðŸ–¼ï¸ðŸ“„ðŸ“Ž]\s*)?\[(?:attached file|archivo adjunto)[^\]]*\]\s*(?:\([^)]+\))?\s*$/gim;

const normalizeQueryForDocumentSearch = (rawQuery: string): string =>
  rawQuery
    .replace(ATTACHMENT_MARKER_PATTERN, '')
    .replace(/^âš ï¸.*$/gm, '')
    .trim();

// ---- Text Cleaning ----

/**
 * Clean extracted document text to remove noise and reduce token count.
 * Typically saves 15-30% of tokens without losing semantic content.
 */
export function cleanDocumentText(raw: string): string {
  return raw
    // Remove control characters (keep tabs, newlines, carriage returns)
    .replace(/[\x00-\x08\x0B\x0C\x0E-\x1F]/g, '')
    // Collapse multiple spaces/tabs into single space
    .replace(/[ \t]{2,}/g, ' ')
    // Remove standalone page numbers (e.g. "  12  " or "- 12 -")
    .replace(/^\s*[-â€”]?\s*\d{1,4}\s*[-â€”]?\s*$/gm, '')
    // Remove common header/footer patterns
    .replace(/^\s*(page|pÃ¡gina|pÃ¡g\.?)\s*\d+\s*(of|de)\s*\d+\s*$/gmi, '')
    // Remove repeated separator lines (e.g. "______" or "------")
    .replace(/^\s*[_\-=~*]{5,}\s*$/gm, '')
    // Collapse 3+ consecutive blank lines into 2
    .replace(/\n{3,}/g, '\n\n')
    // Trim each line
    .split('\n')
    .map((line) => line.trim())
    .join('\n')
    // Final trim
    .trim();
}

// ---- Chunking ----

export interface DocumentChunk {
  index: number;
  content: string;
}

/**
 * Split document text into overlapping chunks using paragraph boundaries.
 * Falls back to character-based splitting for oversized paragraphs.
 */
export function chunkDocument(
  text: string,
  chunkSize = CHUNK_SIZE,
  overlap = CHUNK_OVERLAP,
): DocumentChunk[] {
  if (!text || text.length <= chunkSize) {
    return [{ index: 0, content: text }];
  }

  const paragraphs = text.split(/\n\n+/);
  const rawChunks: DocumentChunk[] = [];
  let currentChunk = '';
  let chunkIndex = 0;

  for (const paragraph of paragraphs) {
    // If adding this paragraph exceeds the chunk size, flush current chunk
    if (
      currentChunk.length + paragraph.length + 2 > chunkSize &&
      currentChunk.length > 0
    ) {
      rawChunks.push({ index: chunkIndex++, content: currentChunk.trim() });
      // Carry over tail words as overlap
      const words = currentChunk.split(/\s+/);
      const overlapWordCount = Math.ceil(overlap / 6);
      currentChunk =
        words.slice(-overlapWordCount).join(' ') + '\n\n' + paragraph;
    } else {
      currentChunk = currentChunk
        ? `${currentChunk}\n\n${paragraph}`
        : paragraph;
    }
  }

  if (currentChunk.trim()) {
    rawChunks.push({ index: chunkIndex, content: currentChunk.trim() });
  }

  // Re-split any oversized chunks by character boundary
  const maxAcceptable = chunkSize * 1.5;
  const finalChunks: DocumentChunk[] = [];
  let globalIndex = 0;

  for (const chunk of rawChunks) {
    if (chunk.content.length <= maxAcceptable) {
      finalChunks.push({ index: globalIndex++, content: chunk.content });
    } else {
      const t = chunk.content;
      for (let pos = 0; pos < t.length; pos += chunkSize - overlap) {
        finalChunks.push({
          index: globalIndex++,
          content: t.slice(pos, pos + chunkSize),
        });
      }
    }
  }

  return finalChunks;
}

// ---- Per-Document Index Info ----

export interface IndexedDocument {
  fileName: string;
  totalChars: number;
  totalChunks: number;
  /** Below RAG_THRESHOLD â€“ included in full instead of chunked */
  isSmall: boolean;
  /** Stored only for small documents */
  fullText?: string;
}

// ---- Document RAG Store ----

/**
 * Manages chunked document indices for a single conversation.
 * Small documents are kept in full; large ones are chunked and
 * only the most relevant chunks are retrieved per query.
 */
export class DocumentRAGStore {
  private store = new VectorStore();
  private indexedDocs = new Map<string, IndexedDocument>();

  /**
   * Index a document's text for future retrieval.
   * The text should already be cleaned via `cleanDocumentText`.
   */
  indexDocument(fileName: string, cleanedText: string): IndexedDocument {
    this.removeDocument(fileName);

    const isSmall = cleanedText.length <= RAG_THRESHOLD_CHARS;
    const docInfo: IndexedDocument = {
      fileName,
      totalChars: cleanedText.length,
      totalChunks: 0,
      isSmall,
      fullText: isSmall ? cleanedText : undefined,
    };

    if (!isSmall) {
      const chunks = chunkDocument(cleanedText);
      docInfo.totalChunks = chunks.length;

      for (const chunk of chunks) {
        this.store.addDocument({
          id: `doc::${fileName}::${chunk.index}`,
          conversationId: fileName,
          conversationTitle: fileName,
          content: chunk.content,
          timestamp: Date.now(),
          role: 'document',
        });
      }
    }

    this.indexedDocs.set(fileName, docInfo);
    return docInfo;
  }

  /** Remove a previously indexed document */
  removeDocument(fileName: string): void {
    this.store.removeConversation(fileName);
    this.indexedDocs.delete(fileName);
  }

  /**
   * Retrieve the most relevant document context for a user query.
   *
   * - Small documents are always included in full.
   * - Large documents contribute only their top-K relevant chunks.
   * - Total output is capped at MAX_RAG_CONTEXT_CHARS.
   */
  getRelevantContext(query: string, topK = DEFAULT_TOP_K): string {
    const parts: string[] = [];
    let totalChars = 0;
    const normalizedQuery = normalizeQueryForDocumentSearch(query || '');

    // 1. Include small documents in full
    for (const [fileName, info] of this.indexedDocs) {
      if (info.isSmall && info.fullText) {
        const section = `[ðŸ“„ ${fileName}]\n${info.fullText}`;
        if (totalChars + section.length <= MAX_RAG_CONTEXT_CHARS) {
          parts.push(section);
          totalChars += section.length;
        }
      }
    }

    // 2. Retrieve relevant chunks from large documents
    const hasLargeDocs = Array.from(this.indexedDocs.values()).some(
      (d) => !d.isSmall,
    );
    if (hasLargeDocs) {
      let appendedFromSearch = false;
      if (normalizedQuery) {
        const results = this.store.search(normalizedQuery, topK, 0.05);
        for (const result of results) {
          const chunkId = result.document.id.split('::').pop() || '?';
          const section =
            `[ðŸ“„ ${result.document.conversationTitle} â€“ fragment ${chunkId}] (relevance: ${result.score.toFixed(2)})\n` +
            result.document.content;
          if (totalChars + section.length > MAX_RAG_CONTEXT_CHARS) break;
          parts.push(section);
          totalChars += section.length;
          appendedFromSearch = true;
        }
      }

      if (!appendedFromSearch) {
        // No effective query (or no matching chunks) â€” include first chunks of each large document.
        for (const [fileName, info] of this.indexedDocs) {
          if (info.isSmall) continue;
          const fallbackResults = this.store.search(fileName, 3, 0.0)
            .filter((result) => result.document.conversationTitle === fileName);
          for (const result of fallbackResults) {
            const section = `[ðŸ“„ ${fileName} â€“ excerpt]\n${result.document.content}`;
            if (totalChars + section.length > MAX_RAG_CONTEXT_CHARS) break;
            parts.push(section);
            totalChars += section.length;
          }
        }
      }
    }

    return parts.join('\n\n---\n\n');
  }

  /** Whether any documents are indexed */
  get hasDocuments(): boolean {
    return this.indexedDocs.size > 0;
  }

  /** List all indexed documents with their stats */
  getDocumentInfo(): IndexedDocument[] {
    return Array.from(this.indexedDocs.values());
  }

  /** Total number of indexed documents */
  get size(): number {
    return this.indexedDocs.size;
  }

  /** Remove everything */
  clear(): void {
    this.store.clear();
    this.indexedDocs.clear();
  }
}

// ---- Conversation Store Registry ----

const conversationStores = new Map<string, DocumentRAGStore>();

/** Get or create the document store for a conversation */
export function getConversationDocStore(
  conversationId: string,
): DocumentRAGStore {
  let store = conversationStores.get(conversationId);
  if (!store) {
    store = new DocumentRAGStore();
    conversationStores.set(conversationId, store);
  }
  return store;
}

/** Discard a conversation's document store */
export function clearConversationDocStore(conversationId: string): void {
  conversationStores.get(conversationId)?.clear();
  conversationStores.delete(conversationId);
}

/** Discard all document stores */
export function clearAllDocStores(): void {
  for (const store of conversationStores.values()) {
    store.clear();
  }
  conversationStores.clear();
}

/**
 * Ensure all text/pdf attachments in a conversation's history are indexed.
 * Safe to call multiple times â€” already-indexed files are skipped.
 */
export function reindexConversationAttachments(
  conversationId: string,
  messages: Array<{
    attachments?: Array<{
      type: string;
      fileName: string;
      textContent?: string;
    }>;
  }>,
): void {
  const store = getConversationDocStore(conversationId);

  for (const msg of messages) {
    if (!msg.attachments) continue;
    for (const att of msg.attachments) {
      if (
        (att.type === 'text' || att.type === 'pdf') &&
        att.textContent &&
        !store['indexedDocs'].has(att.fileName)
      ) {
        store.indexDocument(att.fileName, att.textContent);
      }
    }
  }
}
